{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77d3b9a-82da-445a-8fd6-a26a5830b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "# import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import argparse\n",
    "import re\n",
    "import pickle\n",
    "from helpers import makedir\n",
    "import protopnet\n",
    "import push\n",
    "import prune\n",
    "import numpy as np\n",
    "import save\n",
    "from log import create_logger\n",
    "from preprocess import mean, std, preprocess_input_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5c9eee-d914-45f2-bec9-374f692a2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import cv2\n",
    "\n",
    "# solve potential deadlock https://github.com/pytorch/pytorch/issues/1355\n",
    "cv2.setNumThreads(0)\n",
    "\n",
    "import hydra\n",
    "import torch.backends.cudnn as cudnn\n",
    "import wandb\n",
    "from omegaconf import DictConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from unsup_datasets.lit_dataset import LitDataset\n",
    "from utils.utils import seed_worker\n",
    "import resource\n",
    "rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from hydra import compose, initialize\n",
    "# from omegaconf import OmegaConf\n",
    "\n",
    "# initialize(config_path=\"unsup_configs\", job_name=\"test_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e6da16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2372\n",
      "2372\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# cfg = compose(config_name=\"config.yaml\")\n",
    "# test = OmegaConf.to_yaml(cfg)\n",
    "\n",
    "# # print(os.getcwd())\n",
    "# cfg = dict(cfg)\n",
    "# # if cfg['dataset_name'] is not None:\n",
    "# #     cfg['dataset'] = cfg['dataset_name']\n",
    "# num_classes = 20\n",
    "# cfg['proto_num_classes'] = num_classes\n",
    "# # Entered my username\n",
    "# wandb.init(project='unsup-parts', entity='kadziolka-marcin', mode='disabled', allow_val_change=True)\n",
    "# wandb.config.update(cfg, allow_val_change=True)\n",
    "# args = wandb.config\n",
    "# cudnn.enabled = True\n",
    "num_classes = 20\n",
    "# if args.exp_name is not None:\n",
    "#     api = wandb.Api()\n",
    "#     run = api.run(path=f'wandb_userid/unsup-parts/{wandb.run.id}')\n",
    "#     run.name = f'{args.exp_name}-{run.name}'\n",
    "#     run.save()\n",
    "\n",
    "# print(\"---------------------------------------\")\n",
    "# print(f\"Arguments received: \")\n",
    "# print(\"---------------------------------------\")\n",
    "# for k, v in sorted(args.items()):\n",
    "#     print(f\"{k:25}: {v}\")\n",
    "# print(\"---------------------------------------\")\n",
    "\n",
    "\n",
    "# OURS\n",
    "# SETTINGS FILE\n",
    "\n",
    "# our default architecture\n",
    "# it is used in ppnet class to download pretrained resnet34\n",
    "base_architecture = 'resnet34'\n",
    "img_size = 224\n",
    "\n",
    "# 4 prototypes per 1 class, 20 classes == 80 prototypes\n",
    "prototype_shape = (80, 128, 1, 1)\n",
    "\n",
    "prototype_activation_function = 'log'\n",
    "add_on_layers_type = 'regular'\n",
    "\n",
    "train_batch_size = 20\n",
    "test_batch_size = 20\n",
    "train_push_batch_size = 20\n",
    "\n",
    "from datasets.PartsDataLoader import PDL\n",
    "root = './'\n",
    "\n",
    "normalize = transforms.Normalize(mean=mean,\n",
    "                                 std=std)\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize # very important\n",
    "])\n",
    "\n",
    "trans_push = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # don't normalize push loader!\n",
    "])\n",
    "\n",
    "\n",
    "ds_train = PDL(root, train=True, transform=trans, num_classes=num_classes, images=[\"images\", \"rotated\", \"sheared\", \"skewed\"])\n",
    "print(len(ds_train))\n",
    "ds_train_push = PDL(root, train=True, transform=trans_push, num_classes=num_classes, images=[\"images\", \"rotated\", \"sheared\", \"skewed\"])\n",
    "print(len(ds_train_push))\n",
    "ds_test = PDL(root, train=False, transform=trans, num_classes=num_classes, images=['images'])#['cropped'])\n",
    "print(len(ds_test))\n",
    "\n",
    "train_push_loader = torch.utils.data.DataLoader(ds_train_push, batch_size=train_push_batch_size, shuffle=True, num_workers=2)\n",
    "train_loader = torch.utils.data.DataLoader(ds_train, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(ds_test, batch_size=test_batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e132fcb9-4fcf-49e4-9e48-a202a02b93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data, label in train_loader:\n",
    "#     img = data['IMG'][0]\n",
    "#     parts = data['PART_0'][0], data['PART_1'][0], data['PART_2'][0], data['PART_3'][0]\n",
    "#     # print images using matplotlib\n",
    "    \n",
    "#     r_index = 0\n",
    "#     f, axarr = plt.subplots(1, 5, figsize=(20,15))\n",
    "#     axarr[0].imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "#     axarr[1].imshow(np.transpose(parts[0].numpy(), (1, 2, 0)), cmap=\"gray\")\n",
    "#     axarr[2].imshow(np.transpose(parts[1].numpy(), (1, 2, 0)), cmap=\"gray\")\n",
    "#     axarr[3].imshow(np.transpose(parts[2].numpy(), (1, 2, 0)), cmap=\"gray\")\n",
    "#     axarr[4].imshow(np.transpose(parts[3].numpy(), (1, 2, 0)), cmap=\"gray\");\n",
    "#     plt.title(label[0])\n",
    "#     plt.show()\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936c8e3d-7a18-4b8b-91f6-78f952d921d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ours_classes = {}\n",
    "# needed_classes = list(range(20))\n",
    "# print(needed_classes)\n",
    "# for data, labels in test_loader:\n",
    "#     image = data['IMG']\n",
    "#     for i in range(len(labels)):\n",
    "#         if labels[i] in needed_classes:\n",
    "#             ours_classes[labels[i]] = image[i]\n",
    "#             needed_classes.remove(labels[i])\n",
    "# print(ours_classes.keys())\n",
    "# print(needed_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "debe9c6a-e3f7-44e7-a477-c4d31c6fcc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(4, 5, figsize=(10, 10))\n",
    "# for idx, (label, image) in enumerate(sorted(ours_classes.items())):\n",
    "#     i = idx//5\n",
    "#     j = idx%5\n",
    "#     axs[i, j].imshow(np.transpose(image, (1, 2, 0)))\n",
    "#     axs[i, j].set_title(label.numpy())\n",
    "#     axs[i, j].axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e80d5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "model_dir = f\"PDL_ProtoPNet_base_arch_{base_architecture}_n_classes_{num_classes}_prot_shape_{prototype_shape}_{dt_string}\"\n",
    "model_dir = \"TESTING\"\n",
    "makedir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f88667d-ebb7-43cc-8d8a-69545910d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log, logclose = create_logger(log_filename=os.path.join(model_dir, 'train.log'))\n",
    "img_dir = os.path.join(model_dir, 'img')\n",
    "makedir(img_dir)\n",
    "weight_matrix_filename = 'outputL_weights'\n",
    "prototype_img_filename_prefix = 'prototype-img'\n",
    "prototype_self_act_filename_prefix = 'prototype-self-act'\n",
    "proto_bound_boxes_filename_prefix = 'bb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9994b92c-9099-4186-b3e0-c979fc58afd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 2372\n",
      "test set size: 512\n"
     ]
    }
   ],
   "source": [
    "log('training set size: {0}'.format(len(train_loader.dataset)))\n",
    "# log('push set size: {0}'.format(len(train_push_loader.dataset)))\n",
    "log('test set size: {0}'.format(len(test_loader.dataset)))\n",
    "# log('train batch size: {0}'.format(train_batch_size))\n",
    "\n",
    "# print(len(train_loader.dataset))\n",
    "# print(len(train_push_loader.dataset))\n",
    "# print(len(test_loader.dataset))\n",
    "# print(train_batch_size)\n",
    "\n",
    "# construct the model\n",
    "ppnet = protopnet.construct_PPNet(base_architecture=base_architecture,\n",
    "                              pretrained=True, img_size=img_size,\n",
    "                              prototype_shape=prototype_shape,\n",
    "                              num_classes=num_classes,\n",
    "                              prototype_activation_function=prototype_activation_function,\n",
    "                              add_on_layers_type=add_on_layers_type)\n",
    "\n",
    "ppnet = ppnet.cuda()\n",
    "ppnet_multi = torch.nn.DataParallel(ppnet)\n",
    "class_specific = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d5782e7-bf3f-4ed1-8ed0-c8d69aa78da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "\n",
    "joint_optimizer_lrs = {'features': 1e-4,\n",
    "                       'add_on_layers': 3e-3,\n",
    "                       'prototype_vectors': 3e-3}\n",
    "joint_lr_step_size = 5\n",
    "\n",
    "warm_optimizer_lrs = {'add_on_layers': 3e-3,\n",
    "                      'prototype_vectors': 3e-3}\n",
    "\n",
    "last_layer_optimizer_lr = 1e-4\n",
    "\n",
    "coefs = {\n",
    "    'crs_ent': 1,\n",
    "    'clst': 0.8,\n",
    "    'sep': -0.08,\n",
    "    'l1': 1e-4,\n",
    "}\n",
    "\n",
    "joint_optimizer_specs = \\\n",
    "[{'params': ppnet.features.parameters(), 'lr': joint_optimizer_lrs['features'], 'weight_decay': 1e-3}, # bias are now also being regularized\n",
    " {'params': ppnet.add_on_layers.parameters(), 'lr': joint_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    " {'params': ppnet.prototype_vectors, 'lr': joint_optimizer_lrs['prototype_vectors']},\n",
    "]\n",
    "joint_optimizer = torch.optim.Adam(joint_optimizer_specs)\n",
    "joint_lr_scheduler = torch.optim.lr_scheduler.StepLR(joint_optimizer, step_size=joint_lr_step_size, gamma=0.1)\n",
    "\n",
    "\n",
    "warm_optimizer_specs = \\\n",
    "[{'params': ppnet.add_on_layers.parameters(), 'lr': warm_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    " {'params': ppnet.prototype_vectors, 'lr': warm_optimizer_lrs['prototype_vectors']},\n",
    "]\n",
    "warm_optimizer = torch.optim.Adam(warm_optimizer_specs)\n",
    "\n",
    "\n",
    "last_layer_optimizer_specs = [{'params': ppnet.last_layer.parameters(), 'lr': last_layer_optimizer_lr}]\n",
    "last_layer_optimizer = torch.optim.Adam(last_layer_optimizer_specs)\n",
    "\n",
    "# weighting of different training losses\n",
    "\n",
    "\n",
    "\n",
    "# number of training epochs, number of warm epochs, push start epoch, push epochs\n",
    "num_train_epochs = 100\n",
    "num_warm_epochs = 5\n",
    "push_start = 10\n",
    "push_epochs = [i for i in range(num_train_epochs) if i % 10 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9be24ac0-4298-4a6c-9423-9b4053ca2e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_dir=None, model_name=None, ppnet=None, joint_optimizer=None, warm_optimizer=None, last_layer_optimizer=None):\n",
    "    # none everywhere as to not load anything that jupyter holds in memory\n",
    "    model_path = model_dir + '/' + model_name\n",
    "    joint_optimizer_path = model_dir + '/joint_optimizer.pth'\n",
    "    warm_optimizer_path = model_dir + '/warm_optimizer.pth'\n",
    "    last_layer_optimizer_path = model_dir + '/last_layer_optimizer.pth'\n",
    "    variables_path = model_dir + '/variables.pkl'\n",
    "\n",
    "\n",
    "    # WARNING: THEY SAVE WHOLE MODEL, I SAVE DICT STATE\n",
    "    ppnet.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    # Load the optimizer states\n",
    "    joint_optimizer.load_state_dict(torch.load(joint_optimizer_path))\n",
    "    warm_optimizer.load_state_dict(torch.load(warm_optimizer_path))\n",
    "    last_layer_optimizer.load_state_dict(torch.load(last_layer_optimizer_path))\n",
    "    \n",
    "    # It probably isn't that useful, so they are not returned\n",
    "    with open(variables_path, 'rb') as f:\n",
    "        variables = pickle.load(f)\n",
    "    coefs = variables['coefs']\n",
    "    num_train_epochs = variables['num_train_epochs']\n",
    "    num_warm_epochs = variables['num_warm_epochs']\n",
    "    push_start = variables['push_start']\n",
    "    push_epochs = variables['push_epochs']\n",
    "    class_specific = variables['class_specific']\n",
    "\n",
    "\n",
    "    \n",
    "    # Load other variables as needed\n",
    "    log(f\"Model and optimizers loaded from {model_dir}\")\n",
    "    log(f\"Model: {model_path}\")\n",
    "    log(f\"Joint optimizer: {joint_optimizer_path}\")\n",
    "    log(f\"Warm optimizer: {warm_optimizer_path}\")\n",
    "    log(f\"Last layer_optimizer: {last_layer_optimizer_path}\")\n",
    "    # log(f\"Variables: {variables_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2502a86d-e6a5-42fc-a9a2-66505695518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and optimizers loaded from ProtoPNet_base_arch_resnet34_n_classes_20_prot_shape_(80, 128, 1, 1)_28_05_2023_15_15_19\n",
      "Model: ProtoPNet_base_arch_resnet34_n_classes_20_prot_shape_(80, 128, 1, 1)_28_05_2023_15_15_19/ProtoPNet_base_arch_resnet34_n_classes_20_prot_shape_(80, 128, 1, 1)_28_05_2023_15_15_19_0.8019417475728156.pth\n",
      "Joint optimizer: ProtoPNet_base_arch_resnet34_n_classes_20_prot_shape_(80, 128, 1, 1)_28_05_2023_15_15_19/joint_optimizer.pth\n",
      "Warm optimizer: ProtoPNet_base_arch_resnet34_n_classes_20_prot_shape_(80, 128, 1, 1)_28_05_2023_15_15_19/warm_optimizer.pth\n",
      "Last layer_optimizer: ProtoPNet_base_arch_resnet34_n_classes_20_prot_shape_(80, 128, 1, 1)_28_05_2023_15_15_19/last_layer_optimizer.pth\n"
     ]
    }
   ],
   "source": [
    "load_model_dir = 'ProtoPNet_base_arch_resnet34_n_classes_20_prot_shape_(80, 128, 1, 1)_28_05_2023_15_15_19'\n",
    "load_model_name = 'ProtoPNet_base_arch_resnet34_n_classes_20_prot_shape_(80, 128, 1, 1)_28_05_2023_15_15_19_0.8019417475728156.pth'\n",
    "load_model(model_dir=load_model_dir, model_name=load_model_name, ppnet=ppnet, joint_optimizer=joint_optimizer, warm_optimizer=warm_optimizer, last_layer_optimizer=last_layer_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ea030f3-18ae-404c-b80f-49675d85eeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "epoch: \t0\n",
      "\twarm\n",
      "\ttrain\n",
      "\ttime: \t23.13093400001526\n",
      "\tcross ent: \t1.0265217165486151\n",
      "\tcluster: \t0.21048224803467258\n",
      "\tseparation:\t0.06734309064570673\n",
      "\tavg separation:\t1.8611693111788326\n",
      "\t accu: \t\t72.5548060708263%\n",
      "\tl1: \t\t806.6661376953125\n",
      "\tp dist pair: \t4.2457475662231445\n",
      "\t total parts loss: \t697.19775390625\n",
      "\ttest\n",
      "\ttime: \t4.271228075027466\n",
      "\tcross ent: \t1.1262193402418723\n",
      "\tcluster: \t0.21753089960951072\n",
      "\tseparation:\t0.03157394157292751\n",
      "\tavg separation:\t2.1248281643940854\n",
      "\t accu: \t\t71.6796875%\n",
      "\tl1: \t\t806.6661376953125\n",
      "\tp dist pair: \t4.2457475662231445\n",
      "\t total parts loss: \t545.233642578125\n",
      "epoch: \t1\n",
      "\twarm\n",
      "\ttrain\n",
      "\ttime: \t22.717811346054077\n",
      "\tcross ent: \t0.5845248729240995\n",
      "\tcluster: \t0.13705217985420667\n",
      "\tseparation:\t0.04622970137964277\n",
      "\tavg separation:\t2.205498791542374\n",
      "\t accu: \t\t83.13659359190557%\n",
      "\tl1: \t\t806.6661376953125\n",
      "\tp dist pair: \t4.495097637176514\n",
      "\t total parts loss: \t632.024658203125\n",
      "\ttest\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112641/3130546266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                       class_specific=class_specific, coefs=coefs, log=log)\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     accu = utnt.test(model=ppnet_multi, dataloader=test_loader,\n\u001b[0m\u001b[1;32m     20\u001b[0m                     class_specific=class_specific, log=log)\n\u001b[1;32m     21\u001b[0m     save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + 'nopush', accu=accu,\n",
      "\u001b[0;32m~/Desktop/uni/birds/my_protopnet/code/unsup_train_and_test.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, dataloader, class_specific, log)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\ttest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     return _train_or_test(model=model, dataloader=dataloader, optimizer=None,\n\u001b[0m\u001b[1;32m    220\u001b[0m                           class_specific=class_specific, log=log)\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/uni/birds/my_protopnet/code/unsup_train_and_test.py\u001b[0m in \u001b[0;36m_train_or_test\u001b[0;34m(model, dataloader, optimizer, class_specific, use_l1_mask, coefs, log, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mactivation_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mactivation_masks_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "import unsup_train_and_test as utnt\n",
    "target_acc = 0.90\n",
    "log('start training')\n",
    "import copy\n",
    "for epoch in range(num_train_epochs):\n",
    "    log('epoch: \\t{0}'.format(epoch))\n",
    "\n",
    "    if epoch < num_warm_epochs:\n",
    "        utnt.warm_only(model=ppnet_multi, log=log)\n",
    "        _ = utnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=warm_optimizer,\n",
    "                      class_specific=class_specific, coefs=coefs, log=log)\n",
    "    else:\n",
    "        utnt.joint(model=ppnet_multi, log=log)\n",
    "        joint_lr_scheduler.step()\n",
    "        _ = utnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=joint_optimizer,\n",
    "                      class_specific=class_specific, coefs=coefs, log=log)\n",
    "\n",
    "    accu = utnt.test(model=ppnet_multi, dataloader=test_loader,\n",
    "                    class_specific=class_specific, log=log)\n",
    "    save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + 'nopush', accu=accu,\n",
    "                                target_accu=target_acc, log=log)\n",
    "\n",
    "    if epoch >= push_start and epoch in push_epochs:\n",
    "        push.push_prototypes(\n",
    "            train_push_loader, # pytorch dataloader (must be unnormalized in [0,1])\n",
    "            prototype_network_parallel=ppnet_multi, # pytorch network with prototype_vectors\n",
    "            class_specific=class_specific,\n",
    "            preprocess_input_function=preprocess_input_function, # normalize if needed\n",
    "            prototype_layer_stride=1,\n",
    "            root_dir_for_saving_prototypes=img_dir, # if not None, prototypes will be saved here\n",
    "            epoch_number=epoch, # if not provided, prototypes saved previously will be overwritten\n",
    "            prototype_img_filename_prefix=prototype_img_filename_prefix,\n",
    "            prototype_self_act_filename_prefix=prototype_self_act_filename_prefix,\n",
    "            proto_bound_boxes_filename_prefix=proto_bound_boxes_filename_prefix,\n",
    "            save_prototype_class_identity=True,\n",
    "            log=log)\n",
    "        accu = utnt.test(model=ppnet_multi, dataloader=test_loader,\n",
    "                        class_specific=class_specific, log=log)\n",
    "        save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + 'push', accu=accu,\n",
    "                                    target_accu=target_acc, log=log)\n",
    "\n",
    "        if prototype_activation_function != 'linear':\n",
    "            utnt.last_only(model=ppnet_multi, log=log)\n",
    "            for i in range(20):\n",
    "                log('iteration: \\t{0}'.format(i))\n",
    "                _ = utnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=last_layer_optimizer,\n",
    "                              class_specific=class_specific, coefs=coefs, log=log)\n",
    "                accu = utnt.test(model=ppnet_multi, dataloader=test_loader,\n",
    "                                class_specific=class_specific, log=log)\n",
    "                save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + '_' + str(i) + 'push', accu=accu,\n",
    "                                            target_accu=target_acc, log=log)\n",
    "   \n",
    "logclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce38468-18a0-4ee5-a062-92de61e5f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVING MODEL TO TRAIN LATER\n",
    "\n",
    "# # Save the model state\n",
    "# model_path = model_dir + '/' + model_dir + '_' + str(accu) + '.pth'\n",
    "# torch.save(ppnet.state_dict(), model_path)\n",
    "\n",
    "# # Save the optimizer states\n",
    "# joint_optimizer_path = model_dir + '/joint_optimizer.pth'\n",
    "# torch.save(joint_optimizer.state_dict(), joint_optimizer_path)\n",
    "\n",
    "# warm_optimizer_path = model_dir + '/warm_optimizer.pth'\n",
    "# torch.save(warm_optimizer.state_dict(), warm_optimizer_path)\n",
    "\n",
    "# last_layer_optimizer_path = model_dir + '/last_layer_optimizer.pth'\n",
    "# torch.save(last_layer_optimizer.state_dict(), last_layer_optimizer_path)\n",
    "\n",
    "# # Save other variables\n",
    "# variables_path = model_dir + '/variables.pkl'\n",
    "# variables = {\n",
    "#     'coefs': coefs,\n",
    "#     'num_train_epochs': num_train_epochs,\n",
    "#     'num_warm_epochs': num_warm_epochs,\n",
    "#     'push_start': push_start,\n",
    "#     'push_epochs': push_epochs,\n",
    "#     'class_specific': class_specific,\n",
    "#     # Include other variables you want to save\n",
    "# }\n",
    "# with open(variables_path, 'wb') as f:\n",
    "#     pickle.dump(variables, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fffafb-11a9-4339-b5a5-a83dc97b9d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
